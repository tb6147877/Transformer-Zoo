{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1.Preparations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41d57aa6af6cde42"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-22T15:09:48.865090500Z",
     "start_time": "2026-02-22T15:09:44.875045800Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ROOT = \"/content/Images\"\n",
    "ANN_FILE = \"/content/captions.txt\"\n",
    "\n",
    "def to_2tuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return x\n",
    "    return (x, x)\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def clear_cuda():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "def tensor_to_device(*tensors, device=torch.device(\"cpu\"), non_blocking=True):\n",
    "    moved = tuple(t.to(device, non_blocking=non_blocking) for t in tensors)\n",
    "    return moved if len(moved) > 1 else moved[0]\n",
    "\n",
    "def get_ctx(use_mixed: bool, device: torch.device, amp_mode: str = \"auto\"):\n",
    "    if not use_mixed or amp_mode == \"off\":\n",
    "        print(\"Not using autocast context\")\n",
    "        return nullcontext()\n",
    "\n",
    "    device_type = device.type\n",
    "\n",
    "    if amp_mode == \"fp16\":\n",
    "        dtype = torch.float16\n",
    "    elif amp_mode == \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "    else:\n",
    "        if device_type == \"cuda\":\n",
    "            dtype = torch.bfloat16\n",
    "        elif device_type == \"mps\":\n",
    "            dtype = torch.float16\n",
    "        elif device_type == \"cpu\":\n",
    "            dtype = torch.float16\n",
    "        else:\n",
    "            return nullcontext()\n",
    "\n",
    "    print(f\"Using autocast with dtype={dtype} on device type={device_type}\")\n",
    "    return torch.autocast(device_type=device_type, dtype=dtype)\n",
    "\n",
    "\n",
    "def load_tokenizer(model_name=\"openai-community/gpt2-medium\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def clean_caption(caption: str) -> str:\n",
    "    caption = caption.strip()\n",
    "    caption = re.sub(r\"\\s+\", \" \", caption)\n",
    "    caption = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", caption)\n",
    "\n",
    "    return caption\n",
    "\n",
    "def show_samples(\n",
    "    dataset,\n",
    "    nrows=2,\n",
    "    ncols=2,\n",
    "):\n",
    "    num_samples = nrows * ncols\n",
    "    idxs = random.sample(range(len(dataset)), k=num_samples)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 4 * nrows))\n",
    "\n",
    "    for ax, idx in zip(axes.flatten(), idxs):\n",
    "        sample = dataset[idx]\n",
    "        if isinstance(sample, tuple):\n",
    "            img_t, cap = sample\n",
    "        else:\n",
    "            img_t, cap = sample[\"pixel_values\"], sample[\"caption\"]\n",
    "\n",
    "        # Convert CHW tensor to HWC numpy\n",
    "        if torch.is_tensor(img_t):\n",
    "            img_np = img_t.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "        else:  # PIL Image\n",
    "            img_np = transforms.ToTensor()(img_t).numpy().transpose(1, 2, 0)\n",
    "\n",
    "        ax.imshow(img_np)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(cap[:40] + (\"…\" if len(cap) > 40 else \"\"))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-22T15:09:52.376492Z",
     "start_time": "2026-02-22T15:09:52.330562800Z"
    }
   },
   "id": "4ce42dca7e5b3dc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.CLIP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf48952985fef961"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    # Vision Modeling\n",
    "    vit_hf_model_name:str = \"google/vit-base-patch16-224\"\n",
    "    vit_image_size:int = 224\n",
    "    vit_patch_size:int = 16\n",
    "    vit_hidden_size:int = 768 # 16*16*3\n",
    "    vit_num_hidden_layers:int = 12\n",
    "    vit_num_attention_heads:int = 12\n",
    "    vit_intermediate_size:int = 3072\n",
    "    vit_layer_norm_eps:float = 1e-12\n",
    "    vit_dropout:float=0.1\n",
    "    \n",
    "    #Language Modeling\n",
    "    lm_hf_model_name:str=\"openai-community/gpt2-medium\"\n",
    "    lm_vocab_size:int = 50257\n",
    "    lm_n_layer:int = 24\n",
    "    lm_n_head:int = 16\n",
    "    lm_n_embd:int = 1024\n",
    "    lm_n_positions:int = 76\n",
    "    lm_resid_pdrop:float=0.1\n",
    "    lm_embd_pdrop:float=0.1\n",
    "    lm_attn_pdrop:float=0.1\n",
    "    lm_layer_norm_epsilon:float=1e-5\n",
    "    lm_tie_word_embeddings:bool=True\n",
    "    \n",
    "    lm_eot_token_id:int = 50256\n",
    "    lm_pad_token_id:int = (50256) # If you set pad_token_id, you need to add it to tokenizer, and resize model embeddings\n",
    "    \n",
    "    #Projection\n",
    "    project_dim:int = 512\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-22T15:09:54.637856Z",
     "start_time": "2026-02-22T15:09:54.593968100Z"
    }
   },
   "id": "8704ab624a0e6b5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.1Vision Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "283a5d7e263d5e56"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.vit_hidden_size, config.vit_intermediate_size)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(config.vit_intermediate_size,config.vit_hidden_size)\n",
    "        self.drop = nn.Dropout(config.vit_dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.drop(self.fc2(self.act(self.fc1(x))))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.vit_num_attention_heads\n",
    "        self.head_dim = config.vit_hidden_size // config.vit_num_attention_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.vit_hidden_size, config.vit_hidden_size)\n",
    "        self.k_proj = nn.Linear(config.vit_hidden_size, config.vit_hidden_size)\n",
    "        self.v_proj = nn.Linear(config.vit_hidden_size, config.vit_hidden_size)\n",
    "        \n",
    "        self.proj = nn.Linear(config.vit_hidden_size, config.vit_hidden_size)\n",
    "        self.attn_drop = nn.Dropout(config.vit_dropout)\n",
    "        self.proj_drop = nn.Dropout(config.vit_dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q_proj(x).reshape(B,N,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        k = self.k_proj(x).reshape(B,N,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        v = self.v_proj(x).reshape(B,N,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        attn = (q@k.transpose(-2,-1))*self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        out = (attn@v).transpose(1,2).reshape(B,N,C)\n",
    "        out = self.proj(out)\n",
    "        return self.proj_drop(out)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.vit_hidden_size, eps=config.vit_layer_norm_eps)\n",
    "        self.attn = Attention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.vit_hidden_size, eps=config.vit_layer_norm_eps)\n",
    "        self.mlp = MLP(config)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "# -----------------------\n",
    "# Vision Transformer\n",
    "# -----------------------\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "        \n",
    "        self.patch_embed = nn.Conv2d(3, config.vit_hidden_size, kernel_size=config.vit_patch_size, stride=config.vit_patch_size)\n",
    "        num_patches = (config.vit_image_size//config.vit_patch_size)**2\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,config.vit_hidden_size))\n",
    "        self.pos_embd = nn.Parameter(torch.zeros(1,num_patches+1,config.vit_hidden_size))\n",
    "        self.pos_drop = nn.Dropout(config.vit_dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([Block(config) for _  in range(config.vit_num_hidden_layers)])\n",
    "        self.norm = nn.LayerNorm(config.vit_hidden_size,eps = config.vit_layer_norm_eps)\n",
    "    def forward(self,x):\n",
    "        # x: (B, 3, H, W)\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x) # (B, C, H/P, W/P)\n",
    "        x = x.flatten(2).transpose(1,2) # (B, N, C)\n",
    "        cls = self.cls_token.expand(B,-1,-1) # (B, 1, C)\n",
    "        x = torch.cat((cls,x),dim=1)\n",
    "        x = x + self.pos_embd\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        cls_out = x[:,0]  # CLS token\n",
    "        \n",
    "        return cls_out\n",
    "\n",
    "@torch.no_grad()\n",
    "def load_hf_vit_into_scratch(model:VisionTransformer, hf_name:str = \"google/vit-base-patch16-224\", device:Optional[str]=None):\n",
    "    from transformers import AutoConfig, ViTModel\n",
    "    \n",
    "    hf_cfg = AutoConfig.from_pretrained(hf_name, add_pooling_layer = False)\n",
    "    assert hf_cfg.hidden_size == model.config.vit_hidden_size\n",
    "    assert hf_cfg.num_hidden_layers == model.config.vit_num_hidden_layers\n",
    "    assert hf_cfg.num_attention_heads == model.config.vit_num_attention_heads\n",
    "    assert hf_cfg.intermediate_size == model.config.vit_intermediate_size\n",
    "    assert hf_cfg.image_size == model.config.vit_image_size\n",
    "    assert hf_cfg.patch_size == model.config.vit_patch_size\n",
    "    \n",
    "    hf_model = ViTModel.from_pretrained(hf_name)\n",
    "    hf_sd = hf_model.state_dict()\n",
    "    sd = model.state_dict()\n",
    "    \n",
    "    # patch embed\n",
    "    sd[\"patch_embed.weight\"] = hf_sd[\"embeddings.patch_embeddings.projection.weight\"]\n",
    "    sd[\"patch_embed.bias\"] = hf_sd[\"embeddings.patch_embeddings.projection.bias\"]\n",
    "\n",
    "    sd[\"cls_token\"] = hf_sd[\"embeddings.cls_token\"]\n",
    "    sd[\"pos_embed\"] = hf_sd[\"embeddings.position_embeddings\"]\n",
    "\n",
    "    for i in range(model.config.vit_num_hidden_layers):\n",
    "        p = f\"blocks.{i}\"\n",
    "        h = f\"encoder.layer.{i}\"\n",
    "\n",
    "        sd[f\"{p}.ln1.weight\"] = hf_sd[f\"{h}.layernorm_before.weight\"]\n",
    "        sd[f\"{p}.ln1.bias\"] = hf_sd[f\"{h}.layernorm_before.bias\"]\n",
    "\n",
    "        sd[f\"{p}.attn.q_proj.weight\"] = hf_sd[f\"{h}.attention.attention.query.weight\"]\n",
    "        sd[f\"{p}.attn.q_proj.bias\"] = hf_sd[f\"{h}.attention.attention.query.bias\"]\n",
    "        sd[f\"{p}.attn.k_proj.weight\"] = hf_sd[f\"{h}.attention.attention.key.weight\"]\n",
    "        sd[f\"{p}.attn.k_proj.bias\"] = hf_sd[f\"{h}.attention.attention.key.bias\"]\n",
    "        sd[f\"{p}.attn.v_proj.weight\"] = hf_sd[f\"{h}.attention.attention.value.weight\"]\n",
    "        sd[f\"{p}.attn.v_proj.bias\"] = hf_sd[f\"{h}.attention.attention.value.bias\"]\n",
    "        # but HF stores q,k,v separately, so you’ll need to adapt here\n",
    "        # to keep it simple, load q/k/v separately like in CLIP\n",
    "        # (adjust your Attention class accordingly if you want exact parity)\n",
    "\n",
    "        sd[f\"{p}.ln2.weight\"] = hf_sd[f\"{h}.layernorm_after.weight\"]\n",
    "        sd[f\"{p}.ln2.bias\"] = hf_sd[f\"{h}.layernorm_after.bias\"]\n",
    "\n",
    "        sd[f\"{p}.mlp.fc1.weight\"] = hf_sd[f\"{h}.intermediate.dense.weight\"]\n",
    "        sd[f\"{p}.mlp.fc1.bias\"] = hf_sd[f\"{h}.intermediate.dense.bias\"]\n",
    "        sd[f\"{p}.mlp.fc2.weight\"] = hf_sd[f\"{h}.output.dense.weight\"]\n",
    "        sd[f\"{p}.mlp.fc2.bias\"] = hf_sd[f\"{h}.output.dense.bias\"]\n",
    "\n",
    "    sd[\"norm.weight\"] = hf_sd[\"layernorm.weight\"]\n",
    "    sd[\"norm.bias\"] = hf_sd[\"layernorm.bias\"]\n",
    "    \n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    if device:\n",
    "        model.to(device)\n",
    "    print(\"Loaded HF ViT weights.\")\n",
    "    if missing:\n",
    "        print(\"Missing keys:\", missing)\n",
    "    if unexpected:\n",
    "        print(\"Unexpected keys:\", unexpected)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-22T15:15:34.779938600Z",
     "start_time": "2026-02-22T15:15:34.732009Z"
    }
   },
   "id": "e84256ec7cf3eef8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2Language Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7448cb14e32ed22c"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.gelu(x)\n",
    "    \n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        assert config.lm_n_embd % config.lm_n_head == 0,  \"n_embd must be divisible by n_head\"\n",
    "        self.n_head = config.lm_n_head\n",
    "        self.head_dim = config.lm_n_embd // config.lm_n_head\n",
    "        self.c_attn = nn.Linear(config.lm_n_embd, 3 * config.lm_n_embd, bias=True)#qkv\n",
    "        self.c_proj = nn.Linear(config.lm_n_embd, config.lm_n_embd, bias=True)\n",
    "        self.attn_drop=nn.Dropout(config.lm_attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.lm_resid_pdrop)\n",
    "        \n",
    "        mask = torch.triu(torch.ones(config.lm_n_positions, config.lm_n_positions),1).bool()\n",
    "        self.register_buffer(\"causal_mask\",mask,persistent=False)\n",
    "        \n",
    "    def forward(self, x, attention_mask: Optional[torch.Tensor]=None):\n",
    "        B, T, C = x.size() # (batch, time, channels)\n",
    "        \n",
    "        qkv = self.c_attn(x) # (B, T, 3*C)\n",
    "        q,k,v = qkv.split(C,dim=-1)  # 3 x (B, T, C)\n",
    "        \n",
    "        q = q.view(B,T,self.n_head,self.head_dim).transpose(1,2)\n",
    "        k = k.view(B,T,self.n_head,self.head_dim).transpose(1,2)\n",
    "        v = v.view(B,T,self.n_head,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Scaled dot-product attention with causal mask\n",
    "        att = (q @ k.transpose(-2,-1))/math.sqrt(self.head_dim) # (B, n_head, T, T)\n",
    "        \n",
    "        att = att.masked_fill(self.causal_mask.unsqueeze(0).unsqueeze(0)[..., :T, :T], float(\"-inf\"))\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask[:,None,None,:].to(torch.bool) # (B, 1, 1, T)\n",
    "            att = att.masked_fill(~attention_mask, float(\"-inf\"))\n",
    "            \n",
    "        att = F.softmax(att,dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att@v # (B, n_head, T, head_dim)\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "        y = self.resid_drop(self.c_proj(y))\n",
    "        \n",
    "        return y\n",
    "        \n",
    "class TMLP(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.c_fc = nn.Linear(config.lm_n_embd, 4 * config.lm_n_embd, bias=True)\n",
    "        self.act = GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.lm_n_embd, config.lm_n_embd, bias=True)\n",
    "        self.drop = nn.Dropout(config.lm_resid_pdrop)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class TBlock(nn.Module):\n",
    "    def __init__(self, config:ModelConfig): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(config.lm_n_embd,config.lm_layer_norm_epsilon)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.lm_n_embd,config.lm_layer_norm_epsilon)\n",
    "        self.mlp = TMLP(config)\n",
    "        \n",
    "    def forward(self, x, attention_mask:Optional[torch.Tensor]=None):\n",
    "        x = x + self.attn(self.ln1(x), attention_mask)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x        \n",
    "    \n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.tok_emb = nn.Embedding(config.lm_vocab_size, config.lm_n_embd)\n",
    "        self.pos_emb = nn.Embedding(config.lm_n_positions, config.lm_n_embd) # learnable\n",
    "        self.drop = nn.Dropout(config.lm_embd_pdrop)\n",
    "        self.blocks = nn.ModuleList([TBlock(config) for _ in range(config.lm_n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.lm_n_embd, eps = config.lm_layer_norm_epsilon)\n",
    "        self.lm_head = nn.Linear(config.lm_n_embd, config.lm_vocab_size, bias=False)\n",
    "        \n",
    "        if config.lm_tie_word_embeddings:\n",
    "            self.lm_head.weight = self.tok_emb.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            #GPT-2\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _pos_indices(self, T, device):\n",
    "        return torch.arange(T, device=device).unsqueeze(0) # (1, T)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask:Optional[torch.Tensor]=None):\n",
    "        \"\"\"\n",
    "        input_ids: (B, T) with T <= context_length; must include EOT token id\n",
    "        attention_mask is optional (1 for valid tokens, 0 for pad) — not needed for CLIP pooling,\n",
    "        but accepted for API symmetry.\n",
    "        \"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        \n",
    "        pos = self._pos_indices(T, device)\n",
    "        x = self.tok_emb(input_ids) + self.pos_emb(pos)\n",
    "        x= self.drop(x)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, attention_mask)\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        eot_idx = (input_ids ==self.config.lm_eot_token_id).int().argmax(dim=1) # (B,)\n",
    "        eot_rep = x[torch.arange(B, device=device), eot_idx,:]# (B, hidden)\n",
    "        \n",
    "        return eot_rep\n",
    "        \n",
    "# -----------------------\n",
    "# Checkpoint loader (HF -> this model)\n",
    "# -----------------------\n",
    "def load_from_hf_into_scratch(\n",
    "    model: TextEncoder, hf_name: str = \"openai-community/gpt2-medium\", device: Optional[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Load weights from a Hugging Face GPT-2 LMHead checkpoint into this scratch model.\n",
    "    Fixes Conv1D (in,out) -> Linear (out,in) by transposing c_attn, c_fc, c_proj weights.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from transformers import AutoConfig, GPT2LMHeadModel\n",
    "\n",
    "    def maybe_T(src: torch.Tensor, target_shape: torch.Size) -> torch.Tensor:\n",
    "        # If src is stored as (in, out) but target wants (out, in), transpose.\n",
    "        if src.shape == target_shape:\n",
    "            return src\n",
    "        if src.shape[::-1] == tuple(target_shape):\n",
    "            return src.t()\n",
    "        # Fallback: let load_state_dict complain if it's truly incompatible.\n",
    "        return src\n",
    "\n",
    "    # sanity checks\n",
    "    hf_cfg = AutoConfig.from_pretrained(hf_name)\n",
    "    assert hf_cfg.n_layer == model.config.lm_n_layer\n",
    "    assert hf_cfg.n_head == model.config.lm_n_head\n",
    "    assert hf_cfg.n_embd == model.config.lm_n_embd\n",
    "    assert hf_cfg.vocab_size == model.config.lm_vocab_size\n",
    "\n",
    "    hf_model = GPT2LMHeadModel.from_pretrained(hf_name)\n",
    "    hf_sd = hf_model.state_dict()\n",
    "    sd = model.state_dict()\n",
    "\n",
    "    # embeddings\n",
    "    sd[\"tok_emb.weight\"] = hf_sd[\"transformer.wte.weight\"]\n",
    "    pos_src = hf_sd[\"transformer.wpe.weight\"]\n",
    "    if pos_src.size(0) >= model.config.lm_n_positions:\n",
    "        sd[\"pos_emb.weight\"] = pos_src[: model.config.lm_n_positions]\n",
    "    else:\n",
    "        pad = model.config.lm_n_positions - pos_src.size(0)\n",
    "        sd[\"pos_emb.weight\"] = torch.cat([pos_src, pos_src[-1:].repeat(pad, 1)], dim=0)\n",
    "\n",
    "    C = model.config.lm_n_embd\n",
    "    for i in range(model.config.lm_n_layer):\n",
    "        p = f\"blocks.{i}\"\n",
    "        h = f\"transformer.h.{i}\"\n",
    "\n",
    "        # LayerNorms\n",
    "        sd[f\"{p}.ln1.weight\"] = hf_sd[f\"{h}.ln_1.weight\"]\n",
    "        sd[f\"{p}.ln1.bias\"] = hf_sd[f\"{h}.ln_1.bias\"]\n",
    "        sd[f\"{p}.ln2.weight\"] = hf_sd[f\"{h}.ln_2.weight\"]\n",
    "        sd[f\"{p}.ln2.bias\"] = hf_sd[f\"{h}.ln_2.bias\"]\n",
    "\n",
    "        # Attention: c_attn (fused qkv) and c_proj\n",
    "        # our shapes: c_attn.weight (3C, C), c_proj.weight (C, C)\n",
    "        sd[f\"{p}.attn.c_attn.weight\"] = maybe_T(hf_sd[f\"{h}.attn.c_attn.weight\"], torch.Size([3 * C, C]))\n",
    "        sd[f\"{p}.attn.c_attn.bias\"] = hf_sd[f\"{h}.attn.c_attn.bias\"]\n",
    "        sd[f\"{p}.attn.c_proj.weight\"] = maybe_T(hf_sd[f\"{h}.attn.c_proj.weight\"], torch.Size([C, C]))\n",
    "        sd[f\"{p}.attn.c_proj.bias\"] = hf_sd[f\"{h}.attn.c_proj.bias\"]\n",
    "\n",
    "        # MLP: c_fc (4C, C) and c_proj (C, 4C)\n",
    "        sd[f\"{p}.mlp.c_fc.weight\"] = maybe_T(hf_sd[f\"{h}.mlp.c_fc.weight\"], torch.Size([4 * C, C]))\n",
    "        sd[f\"{p}.mlp.c_fc.bias\"] = hf_sd[f\"{h}.mlp.c_fc.bias\"]\n",
    "        sd[f\"{p}.mlp.c_proj.weight\"] = maybe_T(hf_sd[f\"{h}.mlp.c_proj.weight\"], torch.Size([C, 4 * C]))\n",
    "        sd[f\"{p}.mlp.c_proj.bias\"] = hf_sd[f\"{h}.mlp.c_proj.bias\"]\n",
    "\n",
    "    # final LayerNorm\n",
    "    sd[\"ln_f.weight\"] = hf_sd[\"transformer.ln_f.weight\"]\n",
    "    sd[\"ln_f.bias\"] = hf_sd[\"transformer.ln_f.bias\"]\n",
    "\n",
    "    # lm_head (tied to tok_emb by default). If untied, copy explicitly (no transpose needed).\n",
    "    if not model.config.lm_tie_word_embeddings:\n",
    "        if \"lm_head.weight\" in hf_sd:\n",
    "            sd[\"lm_head.weight\"] = hf_sd[\"lm_head.weight\"]\n",
    "        else:\n",
    "            sd[\"lm_head.weight\"] = hf_sd[\"transformer.wte.weight\"]\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    if device is not None:\n",
    "        model.to(device)\n",
    "\n",
    "    print(\"Loaded HF weights with Conv1D->Linear transposes.\")\n",
    "    if missing:\n",
    "        print(\"Missing keys:\", missing)\n",
    "    if unexpected:\n",
    "        print(\"Unexpected keys:\", unexpected)\n",
    "    return model\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-22T15:18:39.900494Z",
     "start_time": "2026-02-22T15:18:39.889389Z"
    }
   },
   "id": "8f106005c80e6ac5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.3Modality Projector"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6c2b569c364fe92"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class ModalityProjector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.projection = nn.Linear(in_dim, out_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(out_dim)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # Xavier init for linear layers\n",
    "        nn.init.xavier_normal_(self.projection.weight)\n",
    "        nn.init.zeros_(self.projection.bias)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        \n",
    "        return self.layer_norm(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-22T15:18:41.038599200Z",
     "start_time": "2026-02-22T15:18:40.997078800Z"
    }
   },
   "id": "107c2181e67297ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.4Full CLIP Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20f5d2760ee33790"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "        \n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.vision_encoder = VisionTransformer(config)\n",
    "        self.text_encoder = TextEncoder(config)\n",
    "        \n",
    "        self.vision_proj = ModalityProjector(config.vit_hidden_size, config.project_dim)\n",
    "        self.text_proj = ModalityProjector(config.lm_n_embd, config.project_dim)\n",
    "        \n",
    "        self.load_from_state_dict()\n",
    "        \n",
    "    def forward(self, image, input_ids, attention_mask:Optional[torch.Tensor]=None):\n",
    "        image_embeds = self.encode_image(image) # (B, project_dim)\n",
    "        text_embeds = self.encode_text(input_ids, attention_mask)\n",
    "        \n",
    "        return image_embeds,text_embeds\n",
    "        \n",
    "    def encode_image(self,image):\n",
    "        if image.ndim == 3:\n",
    "            image = image.unsqueeze(0)\n",
    "        image_features = self.vision_encoder(image)# (B, vit_hidden_size)\n",
    "        image_embeds = self.vision_proj(image_features)  # (B, project_dim)\n",
    "        \n",
    "        return image_embeds\n",
    "    \n",
    "    def encode_text(self, input_ids, attention_mask:Optional[torch.Tensor]=None,):\n",
    "        if input_ids.ndim==1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "        text_features = self.text_encoder(input_ids, attention_mask)  # (B, lm_n_embd)\n",
    "        text_embeds = self.text_proj(text_features)  # (B, 512)\n",
    "        return text_embeds\n",
    "        \n",
    "    def load_from_state_dict(self,):\n",
    "        load_from_hf_into_scratch(self.text_encoder, hf_name=self.config.lm_hf_model_name)\n",
    "        load_hf_vit_into_scratch(self.vision_encoder, hf_name=self.config.vit_hf_model_name)\n",
    "        print(f\"Loaded Vision Encoder from {self.config.vit_hf_model_name}\")\n",
    "        print(f\"Loaded Text Encoder from {self.config.lm_hf_model_name}\")\n",
    "        \n",
    "        self.freeze_partial_backbone()\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable parameters: {trainable_params} / {total_params}\")\n",
    "        \n",
    "    def freeze_partial_backbone(self, freeze_ratio:float=0.4):\n",
    "        num_layers = len(self.vision_encoder.blocks)\n",
    "        num_freeze = int(num_layers*freeze_ratio)\n",
    "        for layer in self.vision_encoder.blocks[:num_freeze]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad=False\n",
    "                \n",
    "        self.vision_encoder.cls_token.requires_grad=False\n",
    "        self.vision_encoder.pos_embd.requires_grad=False\n",
    "        \n",
    "        for layer in self.vision_encoder.blocks[num_freeze:]:\n",
    "            layer.apply(init_weights)\n",
    "        \n",
    "        num_layers = len(self.text_encoder.blocks)\n",
    "        num_freeze = int(num_layers*freeze_ratio)\n",
    "        \n",
    "        for layer in self.text_encoder.blocks[:num_freeze]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad=False\n",
    "                \n",
    "        for param in self.text_encoder.tok_emb.parameters():\n",
    "            param.requires_grad=False\n",
    "        \n",
    "        for param in self.text_encoder.pos_emb.parameters():\n",
    "            param.requires_grad=False\n",
    "        \n",
    "        for layer in self.text_encoder.blocks[num_freeze:]:\n",
    "            layer.apply(init_weights)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-22T15:18:41.902361600Z",
     "start_time": "2026-02-22T15:18:41.863528400Z"
    }
   },
   "id": "6ff8d5e8876221b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.5Dummy Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f71f84d5142604d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d388bfa4a344adaa463c8a3bd6c56cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1mGPT2LMHeadModel LOAD REPORT\u001B[0m from: openai-community/gpt2-medium\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...23}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001B[3mNotes:\n",
      "- UNEXPECTED\u001B[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HF weights with Conv1D->Linear transposes.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading weights:   0%|          | 0/198 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bc015071875450c95192b5171b66a3a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1mViTModel LOAD REPORT\u001B[0m from: google/vit-base-patch16-224\n",
      "Key                 | Status     | \n",
      "--------------------+------------+-\n",
      "classifier.bias     | UNEXPECTED | \n",
      "classifier.weight   | UNEXPECTED | \n",
      "pooler.dense.weight | MISSING    | \n",
      "pooler.dense.bias   | MISSING    | \n",
      "\n",
      "\u001B[3mNotes:\n",
      "- UNEXPECTED\u001B[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001B[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HF ViT weights.\n",
      "Unexpected keys: ['pos_embed']\n",
      "Loaded Vision Encoder from google/vit-base-patch16-224\n",
      "Loaded Text Encoder from openai-community/gpt2-medium\n",
      "Trainable parameters: 247686400 / 441096960\n",
      "Dummy forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig()\n",
    "device = get_device()\n",
    "\n",
    "tokenizer = load_tokenizer(config.lm_hf_model_name)\n",
    "config.lm_eot_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "text = [\n",
    "     \"a photo of a cat <|endoftext|>\",\n",
    "     \"contrastingly, a dog <|endoftext|>\",\n",
    "]\n",
    "imgs = torch.randn(len(text), 3, config.vit_image_size, config.vit_image_size)\n",
    "\n",
    "output = tokenizer(\n",
    "     text,\n",
    "     padding=\"max_length\",\n",
    "     truncation=True,\n",
    "     max_length=config.lm_n_positions,\n",
    "     return_tensors=\"pt\",\n",
    "     add_special_tokens=True,\n",
    ")\n",
    "\n",
    "input_ids = output[\"input_ids\"]\n",
    "attention_mask = output[\"attention_mask\"]\n",
    "\n",
    "\n",
    "imgs, input_ids, attention_mask = tensor_to_device(imgs, input_ids, attention_mask, device=device)\n",
    "\n",
    "model = CLIPModel(config)\n",
    "model.to(device)\n",
    "\n",
    "image_embeds, text_embeds = model(imgs, input_ids, attention_mask)\n",
    "\n",
    "assert image_embeds.shape == (len(text), config.project_dim)\n",
    "assert text_embeds.shape == (len(text), config.project_dim)\n",
    "print(\"Dummy forward pass successful!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-22T15:18:50.213174300Z",
     "start_time": "2026-02-22T15:18:42.877824300Z"
    }
   },
   "id": "b7db32d4d0f43908"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "73b48accbc135b38"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

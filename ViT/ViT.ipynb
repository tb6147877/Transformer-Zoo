{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1.Preparations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb120bfbfb99fe4c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:32.715980900Z",
     "start_time": "2026-02-12T16:07:30.700696700Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rich\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_device(verbose: bool = False) -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        if verbose:\n",
    "            print(\"Using GPU\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        if verbose:\n",
    "            print(\"Using Apple Silicon GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if verbose:\n",
    "            print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def clear_cuda():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "def tensor_to_device(*tensors, device: torch.device = torch.device(\"cpu\"), non_blocking=True):\n",
    "    moved = tuple(t.to(device, non_blocking=non_blocking) for t in tensors)\n",
    "    return moved if len(moved) > 1 else moved[0]\n",
    "\n",
    "\n",
    "def print_color(text: str, color: str = \"green\"):\n",
    "    rich.print(f\"[{color}]{text}[/{color}]\")\n",
    "\n",
    "\n",
    "def get_ctx(use_mixed: bool, device: torch.device, amp_mode: str = \"auto\"):\n",
    "    if not use_mixed or amp_mode == \"off\":\n",
    "        print(\"Not using autocast context\")\n",
    "        return nullcontext()\n",
    "\n",
    "    device_type = device.type\n",
    "\n",
    "    if amp_mode == \"fp16\":\n",
    "        dtype = torch.float16\n",
    "    elif amp_mode == \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "    else:\n",
    "        if device_type == \"cuda\":\n",
    "            dtype = torch.bfloat16\n",
    "        elif device_type == \"mps\":\n",
    "            dtype = torch.float16\n",
    "        elif device_type == \"cpu\":\n",
    "            dtype = torch.float16\n",
    "        else:\n",
    "            return nullcontext()\n",
    "\n",
    "    print(f\"Using autocast with dtype={dtype} on device type={device_type}\")\n",
    "    return torch.autocast(device_type=device_type, dtype=dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:33.668558Z",
     "start_time": "2026-02-12T16:07:33.605774700Z"
    }
   },
   "id": "b66a9c0b7220b697"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.Vision Transformer Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "233172ee443d8219"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.1Model Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab4f8c8504b284b2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    image_size : int = 32\n",
    "    patch_size:int = 4\n",
    "    num_channels:int=3\n",
    "    num_classes:int=10\n",
    "    \n",
    "    num_layers:int=6\n",
    "    num_heads:int=8\n",
    "    d_model:int =128\n",
    "    d_ff:int = 512\n",
    "    \n",
    "    dropout_rate:float=0.1\n",
    "    attention_dropout_rate:float=0.1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:35.259953300Z",
     "start_time": "2026-02-12T16:07:35.254952300Z"
    }
   },
   "id": "5d5c26df17e13f46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2Patch Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2b4c5bd1d43416c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class PatchEmbedder(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_patches_per_side = config.image_size//config.patch_size\n",
    "        self.num_patches = self.num_patches_per_side ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels= config.d_model,\n",
    "            kernel_size=config.patch_size,\n",
    "            stride=config.patch_size,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        #x: (B,C,H,W)\n",
    "        x = self.proj(x) # (B,D,H/P,W/P)\n",
    "        x = x.flatten(2) #(B,D,N)\n",
    "        x = x.transpose(1,2) #(B,N,D)\n",
    "        \n",
    "        return x\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:36.364955Z",
     "start_time": "2026-02-12T16:07:36.364955Z"
    }
   },
   "id": "d964c2b5467e912c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.3Position Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9742403d730a65b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class PosEmbedder(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.randn(1, (config.image_size // config.patch_size) ** 2 + 1, config.d_model)\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.cls_token=nn. Parameter(torch.randn(1,1,config.d_model))\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B,1,-1)\n",
    "        x = torch.cat((cls_tokens,x),dim=1)\n",
    "        \n",
    "        x=x+self.position_embeddings\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:37.381476200Z",
     "start_time": "2026-02-12T16:07:37.372427600Z"
    }
   },
   "id": "a55d38a64ccac57a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.4MHA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "803f1e138a6c74be"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads=config.num_heads\n",
    "        self.d_model=config.d_model\n",
    "        assert config.d_model % config.num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_k = config.d_model // config.num_heads\n",
    "        self.qkv_linear = nn.Linear(config.d_model, config.d_model*3)\n",
    "        self.out_linear = nn.Linear(config.d_model,config.d_model)\n",
    "        \n",
    "        self.attention_dropout = nn.Dropout(config.attention_dropout_rate)\n",
    "    \n",
    "    def forward(self,x: torch.Tensor):\n",
    "        B,N,C = x.shape # Batch size, Number of tokens, Embedding dimension\n",
    "        \n",
    "        q,k,v=(\n",
    "            self.qkv_linear(x).reshape(B,N,3,self.num_heads,self.d_k).permute(2,0,3,1,4).unbind(0)\n",
    "        )\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2,-1))/torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)) # (B, num_heads, N, N)\n",
    "        \n",
    "        attn_weight = torch.softmax(scores,dim=-1)# (B, num_heads, N, N)\n",
    "        attn = self.attention_dropout(attn_weight)\n",
    "        \n",
    "        context = torch.matmul(attn,v)# (B, num_heads, N, d_k)\n",
    "        context = context.transpose(1,2).reshape(B,N,C)# (B, N, d_model)\n",
    "        \n",
    "        out =self.out_linear(context)# (B, N, d_model)\n",
    "        \n",
    "        return out, attn_weight\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:38.528576100Z",
     "start_time": "2026-02-12T16:07:38.481266100Z"
    }
   },
   "id": "765267d0865a89c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.5FFN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c43bfd234614945e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model,config.d_ff)\n",
    "        self.fc2 = nn.Linear(config.d_ff,config.d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:39.652276100Z",
     "start_time": "2026-02-12T16:07:39.648276800Z"
    }
   },
   "id": "39a8ba2070e28724"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.6Layer Norm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efb7bcb7d3fdb48c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, in_dim:int, eps:float=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(in_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(in_dim))\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        x_hat = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        \n",
    "        return self.gamma*x_hat+self.beta\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:40.890416300Z",
     "start_time": "2026-02-12T16:07:40.884497400Z"
    }
   },
   "id": "d262c8d4c665d03e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.7ViT Encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f360ab5860475717"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MHA(config)\n",
    "        self.ffn = FFN(config)\n",
    "        \n",
    "        self.norm1 = LayerNorm(config.d_model)\n",
    "        self.norm2 = LayerNorm(config.d_model)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        attn, _ = self.mha(self.norm1(x))\n",
    "        x = x + attn\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:41.932155700Z",
     "start_time": "2026-02-12T16:07:41.932155700Z"
    }
   },
   "id": "dced2465a2e602af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.8Classifier Head"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a24cce7fcf52a6cc"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_model)\n",
    "        self.fc2 = nn.Linear(config.d_model, config.num_classes)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        cls = x[:,0,:]\n",
    "        cls = self.dropout(F.relu(self.fc1(cls)))\n",
    "        cls = self.fc2(cls)\n",
    "        \n",
    "        return cls\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:43.145923100Z",
     "start_time": "2026-02-12T16:07:43.143599800Z"
    }
   },
   "id": "12b74d24fa16ccb5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.9Full ViT Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "334dafbf7a698b20"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.patch_embedder = PatchEmbedder(config)\n",
    "        self.pos_embedder = PosEmbedder(config)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderBlock(config) for _ in range(config.num_layers)])\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.patch_embedder(x)\n",
    "        x = self.pos_embedder(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:44.426050500Z",
     "start_time": "2026-02-12T16:07:44.418240700Z"
    }
   },
   "id": "d3ad748ee82e4bfd"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = Backbone(config)\n",
    "        self.class_head = MLPHead(config)\n",
    "        \n",
    "    def forward(self,x :torch.Tensor):\n",
    "        x = self.backbone(x)\n",
    "        x = self.class_head(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:45.667998900Z",
     "start_time": "2026-02-12T16:07:45.661069900Z"
    }
   },
   "id": "fd867b864c92779a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.10Dummy Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1e1f0dbdfbd38dd"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_device(verbose=True)\n",
    "config = ModelConfig()\n",
    "model = ViT(config).to(DEVICE)\n",
    "dummy_input = torch.randn(2, config.num_channels, config.image_size, config.image_size).to(DEVICE)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "assert dummy_output.shape == (2, config.num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-12T16:07:47.941718100Z",
     "start_time": "2026-02-12T16:07:47.326552300Z"
    }
   },
   "id": "bcb55f98af0a51bf"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "194021426c748bfa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

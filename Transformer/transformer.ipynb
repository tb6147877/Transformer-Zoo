{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "0.Prepare"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22d5d296f7ad6927"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:49:48.762432400Z",
     "start_time": "2026-02-08T21:49:46.341161Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import rich\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_device(verbose: bool = False):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        if verbose:\n",
    "            print(\"Using GPU\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        if verbose:\n",
    "            print(\"Using Apple Silicon GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if verbose:\n",
    "            print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def clear_cuda():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        \n",
    "def tensor_to_device(*tensors, device=\"cpu\", non_blocking=True):\n",
    "    moved = tuple(t.to(device, non_blocking=non_blocking) for t in tensors)\n",
    "    return moved if len(moved) > 1 else moved[0]\n",
    "\n",
    "\n",
    "def print_color(text: str, color: str = \"green\"):\n",
    "    rich.print(f\"[{color}]{text}[/{color}]\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:49:50.417277500Z",
     "start_time": "2026-02-08T21:49:50.362476100Z"
    }
   },
   "id": "7ba160a9530ad6f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.Transformer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bfe09f37a735a74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.1Model Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a62654b4374f302"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 10_000\n",
    "    max_seq_len: int = 128\n",
    "\n",
    "    d_model: int = 512\n",
    "    d_ff: int = 2048\n",
    "    num_heads: int = 8\n",
    "    num_layers: int = 6\n",
    "    dropout: float = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:49:56.792550100Z",
     "start_time": "2026-02-08T21:49:56.792550100Z"
    }
   },
   "id": "9911ad572fe3fe38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2Word Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "febf6a44fa128b17"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size:int, embed_size:int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.embedding(x)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:49:59.789562500Z",
     "start_time": "2026-02-08T21:49:59.785563100Z"
    }
   },
   "id": "30669526b29b7243"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.3Position Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7566b123146697c1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        pos_index = torch.arange(0, config.max_seq_len)\n",
    "        div_term = torch.exp(torch.arange(0, config.d_model,2) * -(math.log(10000.0))/config.d_model)\n",
    "        \n",
    "        pe = torch.zeros(config.max_seq_len, config.d_model).float()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(pos_index.outer(div_term))\n",
    "        pe[:, 1::2] = torch.cos(pos_index.outer(div_term))\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # Shape: (1, max_seq_len, embed_size)\n",
    "        self.register_buffer(\"pe\", pe, persistent=False)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        return self.pe[:, :seq_len, :] # Shape: (1, seq_len, embed_size)\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:50:01.569086800Z",
     "start_time": "2026-02-08T21:50:01.569086800Z"
    }
   },
   "id": "fc710b58fcba6d16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.4Layer Normalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d3e84fb7d60efef"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim:int, eps:float=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "        \n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        x_hat = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        \n",
    "        return self.gamma*x_hat+self.beta\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:50:03.103060800Z",
     "start_time": "2026-02-08T21:50:03.103060800Z"
    }
   },
   "id": "690c33cc2bbafaef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.5Feed Forward Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d15f9b8f04271ebf"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_ff)\n",
    "        self.fc2 = nn.Linear(config.d_ff, config.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:50:04.495580300Z",
     "start_time": "2026-02-08T21:50:04.450050400Z"
    }
   },
   "id": "8dd63e653d29cc39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.6Multi Head Attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdb34c229b4462a5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config:ModelConfig, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.d_model // config.num_heads\n",
    "        self.is_causal = is_causal\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        self.k_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        self.v_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "        \n",
    "    def construct_mask(self, pad_mask, q_len:int, k_len:int, device):\n",
    "        #\"True for valid positions, False for masked positions\"\n",
    "        mask = None\n",
    "        \n",
    "        # causal mask (decoder self-attention only)\n",
    "        if self.is_causal:\n",
    "            mask = torch.tril(torch.ones((q_len,k_len),device=device,dtype=torch.bool))[None, None,:,:]\n",
    "            \n",
    "        if pad_mask is not None:\n",
    "            key_valid = (~pad_mask.bool())[:,None,None,:]\n",
    "            mask = key_valid if mask is None else (mask & key_valid)\n",
    "        \n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        device = q.device\n",
    "        b, q_len, _ = q.shape\n",
    "        _, kv_len,_ = k.shape\n",
    "        mask = self.construct_mask(mask, q_len, kv_len, device)\n",
    "        \n",
    "        q = self.q_proj(q).view(b,q_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        k = self.k_proj(k).view(b,kv_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        v = self.v_proj(v).view(b,kv_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        logits = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            logits = logits.masked_fill(~mask,float(\"-inf\"))\n",
    "            \n",
    "        score = F.softmax(logits,dim=-1)\n",
    "        attn_output = torch.matmul(score,v)\n",
    "        attn_output = einops.rearrange(attn_output,\"batch heads seq_len d_k -> batch seq_len (heads d_k)\",)\n",
    "        \n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output\n",
    "            \n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:50:05.750364100Z",
     "start_time": "2026-02-08T21:50:05.702923Z"
    }
   },
   "id": "6a2b4208986ca05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.7Encoder Block"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "707ceead36fa7463"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(config, is_causal=False)\n",
    "        self.ln1 = LayerNorm(config.d_model)\n",
    "        \n",
    "        self.ffn = FFN(config)\n",
    "        self.ln2 = LayerNorm(config.d_model)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor, mask=None)->torch.Tensor:\n",
    "        attn_output = self.attn(x,x,x,mask=mask) # this means self attention\n",
    "        x = self.ln1(x+attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.ln2(x+ffn_output)\n",
    "        \n",
    "        return x\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:50:06.933042500Z",
     "start_time": "2026-02-08T21:50:06.933042500Z"
    }
   },
   "id": "c9678e3b501cdeda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.8Decoder Block"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "924a8e7858c393cb"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(config, is_causal=False)\n",
    "        self.ln1 = LayerNorm(config.d_model)\n",
    "        \n",
    "        self.cross_attn = MultiHeadAttention(config, is_causal=True)\n",
    "        self.ln2 = LayerNorm(config.d_model)\n",
    "        \n",
    "        self.ffn = FFN(config)\n",
    "        self.ln3 = LayerNorm(config.d_model)\n",
    "        \n",
    "    def forward(\n",
    "            self, \n",
    "            x:torch.Tensor, \n",
    "            enc_output:torch.Tensor, \n",
    "            src_mask=None, \n",
    "            tgt_mask=None)->torch.Tensor:\n",
    "        self_attn_output = self.self_attn(x,x,x,mask=tgt_mask)\n",
    "        x = self.ln1(x+self_attn_output)\n",
    "        \n",
    "        cross_attn_output = self.cross_attn(x,enc_output,enc_output,mask=src_mask)\n",
    "        x = self.ln2(x+cross_attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.ln3(x+ffn_output)\n",
    "        \n",
    "        return x\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:50:08.256340200Z",
     "start_time": "2026-02-08T21:50:08.211085200Z"
    }
   },
   "id": "aa71261e2322d4bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.9Encoder and Decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80b42005e46ae9a1"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(config) for _ in range(config.num_layers)])\n",
    "        \n",
    "    def forward(self, x:torch.Tensor, mask=None)->torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderBlock(config) for _ in range(config.num_layers)])\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            x:torch.Tensor,\n",
    "            enc_output:torch.Tensor,\n",
    "            src_mask=None,\n",
    "            tgt_mask=None)->torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,enc_output,src_mask,tgt_mask)\n",
    "        return x\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:50:09.635580Z",
     "start_time": "2026-02-08T21:50:09.580916300Z"
    }
   },
   "id": "839715e0cccd3bfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.10Full Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c7ff7a906ceb31e"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_embedding = WordEmbedding(config.vocab_size, config.d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(config)\n",
    "        \n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        \n",
    "        self.output_proj = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        self._tie_weights()\n",
    "        \n",
    "    def _tie_weights(self):\n",
    "        self.output_proj.weight = self.vocab_embedding.embedding.weight # their shape is equal\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "            \n",
    "        elif isinstance(module, LayerNorm):\n",
    "            nn.init.ones_(module.gamma)\n",
    "            nn.init.zeros_(module.beta)\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            src_input:torch.Tensor,\n",
    "            tgt_input:torch.Tensor,\n",
    "            src_mask=None,\n",
    "            tgt_mask=None)->torch.Tensor:\n",
    "        src_embeddings = self.vocab_embedding(src_input)*math.sqrt( # give the input a scale\n",
    "            self.vocab_embedding.embedding.embedding_dim\n",
    "        )+self.positional_embedding(src_input)\n",
    "        \n",
    "        tgt_embeddings = self.vocab_embedding(tgt_input)*math.sqrt(\n",
    "            self.vocab_embedding.embedding.embedding_dim\n",
    "        )+self.positional_embedding(tgt_input)\n",
    "        \n",
    "        enc_output = self.encoder(src_embeddings, mask=src_mask)\n",
    "        \n",
    "        dec_output = self.decoder(tgt_embeddings, enc_output,src_mask = src_mask, tgt_mask=tgt_mask)\n",
    "        \n",
    "        logits = self.output_proj(dec_output)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "            \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:51:08.560021500Z",
     "start_time": "2026-02-08T21:51:08.495637Z"
    }
   },
   "id": "42017ff07b4789ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.11A Small Example"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d87b6dafd3121d"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "torch.Size([2, 10, 10000])\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_device(verbose=True)\n",
    "\n",
    "src = torch.randint(0,10000,(2,10)).to(DEVICE)\n",
    "tgt = torch.randint(0,10000,(2,10)).to(DEVICE)\n",
    "\n",
    "config = ModelConfig()\n",
    "\n",
    "model = Transformer(config).to(DEVICE)\n",
    "\n",
    "logits = model(src,tgt)\n",
    "\n",
    "print(logits.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-08T21:51:10.808183700Z",
     "start_time": "2026-02-08T21:51:09.921845500Z"
    }
   },
   "id": "548e253fceba4b1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "34d2df95b3d30747"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

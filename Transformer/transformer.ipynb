{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1.Prepare"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22d5d296f7ad6927"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:24.045482200Z",
     "start_time": "2026-02-09T23:25:21.735338800Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import rich\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_device(verbose: bool = False):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        if verbose:\n",
    "            print(\"Using GPU\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        if verbose:\n",
    "            print(\"Using Apple Silicon GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if verbose:\n",
    "            print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def clear_cuda():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        \n",
    "def tensor_to_device(*tensors, device=\"cpu\", non_blocking=True):\n",
    "    moved = tuple(t.to(device, non_blocking=non_blocking) for t in tensors)\n",
    "    return moved if len(moved) > 1 else moved[0]\n",
    "\n",
    "\n",
    "def print_color(text: str, color: str = \"green\"):\n",
    "    rich.print(f\"[{color}]{text}[/{color}]\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:28.863818200Z",
     "start_time": "2026-02-09T23:25:28.784708Z"
    }
   },
   "id": "7ba160a9530ad6f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.Transformer Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2aaf8aec1d939d9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.1Model Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b00f35fb62a961"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 8_000\n",
    "    max_seq_len: int = 128\n",
    "\n",
    "    d_model: int = 512\n",
    "    d_ff: int = 2048\n",
    "    num_heads: int = 8\n",
    "    num_layers: int = 6\n",
    "    dropout: float = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:33.351554Z",
     "start_time": "2026-02-09T23:25:33.339759600Z"
    }
   },
   "id": "9911ad572fe3fe38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.2Word Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "febf6a44fa128b17"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size:int, embed_size:int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.embedding(x)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:38.578728400Z",
     "start_time": "2026-02-09T23:25:38.562722700Z"
    }
   },
   "id": "30669526b29b7243"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.3Position Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7566b123146697c1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        pos_index = torch.arange(0, config.max_seq_len)\n",
    "        div_term = torch.exp(torch.arange(0, config.d_model,2) * -(math.log(10000.0))/config.d_model)\n",
    "        \n",
    "        pe = torch.zeros(config.max_seq_len, config.d_model).float()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(pos_index.outer(div_term))\n",
    "        pe[:, 1::2] = torch.cos(pos_index.outer(div_term))\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # Shape: (1, max_seq_len, embed_size)\n",
    "        self.register_buffer(\"pe\", pe, persistent=False)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        return self.pe[:, :seq_len, :] # Shape: (1, seq_len, embed_size)\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:39.802454200Z",
     "start_time": "2026-02-09T23:25:39.788836200Z"
    }
   },
   "id": "fc710b58fcba6d16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.4Layer Normalization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d3e84fb7d60efef"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim:int, eps:float=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "        \n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        x_hat = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        \n",
    "        return self.gamma*x_hat+self.beta\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:41.245645800Z",
     "start_time": "2026-02-09T23:25:41.236407400Z"
    }
   },
   "id": "690c33cc2bbafaef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.5Feed Forward Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d15f9b8f04271ebf"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_ff)\n",
    "        self.fc2 = nn.Linear(config.d_ff, config.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:42.788372300Z",
     "start_time": "2026-02-09T23:25:42.780584600Z"
    }
   },
   "id": "8dd63e653d29cc39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.6Multi Head Attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdb34c229b4462a5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config:ModelConfig, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.d_model // config.num_heads\n",
    "        self.is_causal = is_causal\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        self.k_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        self.v_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "        \n",
    "    def construct_mask(self, pad_mask, q_len:int, k_len:int, device):\n",
    "        #\"True for valid positions, False for masked positions\"\n",
    "        mask = None\n",
    "        \n",
    "        # causal mask (decoder self-attention only)\n",
    "        if self.is_causal:\n",
    "            mask = torch.tril(torch.ones((q_len,k_len),device=device,dtype=torch.bool))[None, None,:,:]\n",
    "            \n",
    "        if pad_mask is not None:\n",
    "            key_valid = (~pad_mask.bool())[:,None,None,:]\n",
    "            mask = key_valid if mask is None else (mask & key_valid)\n",
    "        \n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        device = q.device\n",
    "        b, q_len, _ = q.shape\n",
    "        _, kv_len,_ = k.shape\n",
    "        mask = self.construct_mask(mask, q_len, kv_len, device)\n",
    "        \n",
    "        q = self.q_proj(q).view(b,q_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        k = self.k_proj(k).view(b,kv_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        v = self.v_proj(v).view(b,kv_len,self.num_heads,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        logits = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            logits = logits.masked_fill(~mask,float(\"-inf\"))\n",
    "            \n",
    "        score = F.softmax(logits,dim=-1)\n",
    "        attn_output = torch.matmul(score,v)\n",
    "        attn_output = einops.rearrange(attn_output,\"batch heads seq_len d_k -> batch seq_len (heads d_k)\",)\n",
    "        \n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output\n",
    "            \n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:44.226523Z",
     "start_time": "2026-02-09T23:25:44.173277Z"
    }
   },
   "id": "6a2b4208986ca05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.7Encoder Block"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "707ceead36fa7463"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(config, is_causal=False)\n",
    "        self.ln1 = LayerNorm(config.d_model)\n",
    "        \n",
    "        self.ffn = FFN(config)\n",
    "        self.ln2 = LayerNorm(config.d_model)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor, mask=None)->torch.Tensor:\n",
    "        attn_output = self.attn(x,x,x,mask=mask) # this means self attention\n",
    "        x = self.ln1(x+attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.ln2(x+ffn_output)\n",
    "        \n",
    "        return x\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:45.799158900Z",
     "start_time": "2026-02-09T23:25:45.751456Z"
    }
   },
   "id": "c9678e3b501cdeda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.8Decoder Block"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "924a8e7858c393cb"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(config, is_causal=False)\n",
    "        self.ln1 = LayerNorm(config.d_model)\n",
    "        \n",
    "        self.cross_attn = MultiHeadAttention(config, is_causal=True)\n",
    "        self.ln2 = LayerNorm(config.d_model)\n",
    "        \n",
    "        self.ffn = FFN(config)\n",
    "        self.ln3 = LayerNorm(config.d_model)\n",
    "        \n",
    "    def forward(\n",
    "            self, \n",
    "            x:torch.Tensor, \n",
    "            enc_output:torch.Tensor, \n",
    "            src_mask=None, \n",
    "            tgt_mask=None)->torch.Tensor:\n",
    "        self_attn_output = self.self_attn(x,x,x,mask=tgt_mask)\n",
    "        x = self.ln1(x+self_attn_output)\n",
    "        \n",
    "        cross_attn_output = self.cross_attn(x,enc_output,enc_output,mask=src_mask)\n",
    "        x = self.ln2(x+cross_attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.ln3(x+ffn_output)\n",
    "        \n",
    "        return x\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:47.461069100Z",
     "start_time": "2026-02-09T23:25:47.412264200Z"
    }
   },
   "id": "aa71261e2322d4bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.9Encoder and Decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80b42005e46ae9a1"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(config) for _ in range(config.num_layers)])\n",
    "        \n",
    "    def forward(self, x:torch.Tensor, mask=None)->torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderBlock(config) for _ in range(config.num_layers)])\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            x:torch.Tensor,\n",
    "            enc_output:torch.Tensor,\n",
    "            src_mask=None,\n",
    "            tgt_mask=None)->torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,enc_output,src_mask,tgt_mask)\n",
    "        return x\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:49.375696400Z",
     "start_time": "2026-02-09T23:25:49.328304800Z"
    }
   },
   "id": "839715e0cccd3bfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.10Full Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c7ff7a906ceb31e"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_embedding = WordEmbedding(config.vocab_size, config.d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(config)\n",
    "        \n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        \n",
    "        self.output_proj = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        self._tie_weights()\n",
    "        \n",
    "    def _tie_weights(self):\n",
    "        self.output_proj.weight = self.vocab_embedding.embedding.weight # their shape is equal\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "            \n",
    "        elif isinstance(module, LayerNorm):\n",
    "            nn.init.ones_(module.gamma)\n",
    "            nn.init.zeros_(module.beta)\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            src_input:torch.Tensor,\n",
    "            tgt_input:torch.Tensor,\n",
    "            src_mask=None,\n",
    "            tgt_mask=None)->torch.Tensor:\n",
    "        src_embeddings = self.vocab_embedding(src_input)*math.sqrt( # give the input a scale\n",
    "            self.vocab_embedding.embedding.embedding_dim\n",
    "        )+self.positional_embedding(src_input)\n",
    "        \n",
    "        tgt_embeddings = self.vocab_embedding(tgt_input)*math.sqrt(\n",
    "            self.vocab_embedding.embedding.embedding_dim\n",
    "        )+self.positional_embedding(tgt_input)\n",
    "        \n",
    "        enc_output = self.encoder(src_embeddings, mask=src_mask)\n",
    "        \n",
    "        dec_output = self.decoder(tgt_embeddings, enc_output,src_mask = src_mask, tgt_mask=tgt_mask)\n",
    "        \n",
    "        logits = self.output_proj(dec_output)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "            \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:25:51.059531300Z",
     "start_time": "2026-02-09T23:25:50.996147800Z"
    }
   },
   "id": "42017ff07b4789ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.11A Small Example"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d87b6dafd3121d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAcceleratorError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m DEVICE \u001B[38;5;241m=\u001B[39m get_device(verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 3\u001B[0m src \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m tgt \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m10000\u001B[39m,(\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m10\u001B[39m))\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m      6\u001B[0m config \u001B[38;5;241m=\u001B[39m ModelConfig()\n",
      "\u001B[0;31mAcceleratorError\u001B[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_device(verbose=True)\n",
    "\n",
    "src = torch.randint(0,10000,(2,10)).to(DEVICE)\n",
    "tgt = torch.randint(0,10000,(2,10)).to(DEVICE)\n",
    "\n",
    "config = ModelConfig()\n",
    "\n",
    "model = Transformer(config).to(DEVICE)\n",
    "\n",
    "logits = model(src,tgt)\n",
    "\n",
    "print(logits.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T23:26:58.342037500Z",
     "start_time": "2026-02-09T23:26:58.321800300Z"
    }
   },
   "id": "548e253fceba4b1b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.Dataset Preparation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f36360047644688f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.1Download Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e2b48a6b62cd6ce"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b6e4dbb5600490d97ccc1692a47a9cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading readme: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f10e75beba9842db8a48a7d851d3c451"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/27.6M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bad6299271b6403e8863151bf5658c01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/231266 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a70ddcb0369040ada28a94d0205506d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/8549 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ccffdb056a94b68af58b0e673aef4fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/879 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9329ca90f8564c67b99fc9e11cfe7b94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_src.txt, train_tgt.txt saved\n",
      "test_src.txt, test_tgt.txt saved\n"
     ]
    }
   ],
   "source": [
    "def save_dataset(dataset, file_prefix, src=\"en\", tgt=\"zh\", debug=True):\n",
    "    assert file_prefix is not None, \"file_prefix must be set\"\n",
    "    \n",
    "    if debug:\n",
    "        dataset = dataset[:len(dataset)//100]\n",
    "        \n",
    "    if debug:\n",
    "        src_file_name = file_prefix + \"_src_debug\"+\".txt\"\n",
    "        tgt_file_name = file_prefix + \"_tgt_debug\"+\".txt\"\n",
    "    else:\n",
    "        src_file_name = file_prefix + \"_src\"+\".txt\"\n",
    "        tgt_file_name = file_prefix + \"_tgt\"+\".txt\"\n",
    "        \n",
    "    if os.path.exists(src_file_name) and os.path.exists(tgt_file_name):\n",
    "        print(f\"{src_file_name}, {tgt_file_name} already exists\")\n",
    "        return\n",
    "    \n",
    "    with (open(src_file_name, \"w\", encoding=\"utf-8\") as src_f,\n",
    "            open(tgt_file_name, \"w\", encoding=\"utf-8\") as tgt_f,):\n",
    "        for example in dataset:\n",
    "            src_f.write(example[src].strip() + \"\\n\")\n",
    "            tgt_f.write(example[tgt].strip() + \"\\n\")\n",
    "    \n",
    "    print(f\"{src_file_name}, {tgt_file_name} saved\")\n",
    "\n",
    "\n",
    "dataset = load_dataset( \"iwslt2017\", \"iwslt2017-en-zh\", download_mode=\"force_redownload\", trust_remote_code=True, )\n",
    "train_dataset = dataset[\"train\"][\"translation\"]\n",
    "test_dataset = dataset[\"test\"][\"translation\"]\n",
    "\n",
    "save_dataset(train_dataset, file_prefix=\"train\", debug=False)\n",
    "save_dataset(test_dataset, file_prefix=\"test\", debug=False)\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T10:27:11.662253Z",
     "start_time": "2026-02-09T10:27:02.224653800Z"
    }
   },
   "id": "34d2df95b3d30747"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.2Train BPE Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4431b0c19b65db4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer to enzh_bpe_joint.json\n"
     ]
    }
   ],
   "source": [
    "def load_or_train_joint_bpe_tokenizer(\n",
    "        vocab_size:int, \n",
    "        save_prefix:str,\n",
    "        save_name:str=\"bpe_joint.json\",\n",
    "        src_corpus_file:str = \"train_src.txt\",\n",
    "        tgt_corpus_file: str = \"train_tgt.txt\",):\n",
    "    save_path = f\"{save_prefix}_{save_name}\"\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading tokenizer from {save_name}\")\n",
    "        return Tokenizer.from_file(save_path)\n",
    "    \n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.normalizer = NFKC()\n",
    "    tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=True)\n",
    "    tokenizer.decoder = ByteLevelDecoder()\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\n",
    "    \n",
    "    tokenizer.train([src_corpus_file,tgt_corpus_file],trainer)\n",
    "    tokenizer.save(save_path)\n",
    "    \n",
    "    print(f\"Saved tokenizer to {save_path}\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenizer = load_or_train_joint_bpe_tokenizer(\n",
    "    vocab_size=8_000, \n",
    "    save_prefix=\"enzh\", \n",
    "    src_corpus_file=\"train_src.txt\", \n",
    "    tgt_corpus_file=\"train_tgt.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-02-09T14:15:12.374390600Z",
     "start_time": "2026-02-09T14:14:45.960838100Z"
    }
   },
   "id": "7f58a78bf58ee4f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.3Create Dataset Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26b75f035f62f6c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_padding_mask(ids:torch.Tensor, pad_id:int)->torch.Tensor:\n",
    "    return ids == pad_id #True = PAD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98a08417fafab8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, file_prefix, tokenizer, max_seq_len =128, is_debug=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.is_debug = is_debug\n",
    "        \n",
    "        self.pad_id = tokenizer.token_to_id(\"<pad>\") or 0\n",
    "        self.sos_id = tokenizer.token_to_id(\"<s>\") or 1\n",
    "        self.eos_id = tokenizer.token_to_id(\"</s>\") or 2\n",
    "        self.source_texts, self.target_texts = self._load_text_pairs(file_prefix)\n",
    "        \n",
    "    def _load_text_pairs(self, file_prefix):\n",
    "        src_path = file_prefix+\"_src.txt\"\n",
    "        tgt_path = file_prefix+\"_tgt.txt\"\n",
    "        \n",
    "        assert os.path.exists(src_path), f\"Source file not found: {src_path}\"\n",
    "        assert os.path.exists(tgt_path), f\"Target file not found: {src_path}\"\n",
    "        \n",
    "        with open(src_path, encoding=\"utf-8\") as f_src, open(tgt_path, encoding=\"utf-8\") as f_tgt:\n",
    "            src_lines = [line.strip() for line in f_src if line.strip()]\n",
    "            tgt_lines = [line.strip() for line in f_tgt if line.strip()]\n",
    "        \n",
    "        assert len(src_lines) == len(tgt_lines), \"Mismatched number of lines in source and target files\"\n",
    "        \n",
    "        if self.is_debug:\n",
    "            src_lines = src_lines[:1000]\n",
    "            tgt_lines = tgt_lines[:1000]\n",
    "        return src_lines, tgt_lines\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = self.tokenizer.encode(self.source_texts[idx]).ids[:self.max_seq_len]\n",
    "        tgt = self.tokenizer.encode(self.target_texts[idx]).ids[:self.max_seq_len]\n",
    "        \n",
    "        return {\"src_ids\":src, \"tgt_ids\":tgt}\n",
    "    \n",
    "def make_translation_collate_fn(pad_id:int, sos_id:int, eos_id:int, max_len:int|None=None):\n",
    "    def collate(batch):\n",
    "        src_list = [item[\"src_ids\"] for item in batch]\n",
    "        tgt_list = [item[\"tgt_ids\"] for item in batch]\n",
    "        \n",
    "        if max_len is not None:\n",
    "            src_list = [x[:max_len] for x in src_list]\n",
    "            tgt_list = [x[:max_len-1] for x in tgt_list]\n",
    "            \n",
    "        labels_list = [t+[eos_id] for t in tgt_list] # y\n",
    "        decoder_list = [[sos_id]+t for t in tgt_list] # [SOS] + y[:-1]\n",
    "        decoder_list = [d[:len(l)] for d,l in zip(decoder_list, labels_list)]\n",
    "        \n",
    "        src_max = max(len(x) for x in src_list)\n",
    "        dec_max = max(len(x) for x in decoder_list)\n",
    "        \n",
    "        def pad_to(x, L):\n",
    "            return x+[pad_id]*(L-len(x))\n",
    "        \n",
    "        encoder_input_ids = torch.tensor([pad_to(x,src_max) for x in src_list], dtype = torch.long)\n",
    "        decoder_input_ids = torch.tensor([pad_to(x,dec_max) for x in decoder_list], dtype = torch.long)\n",
    "        labels = torch.tensor([pad_to(x,dec_max) for x in labels_list], dtype = torch.long)\n",
    "        \n",
    "        return {\n",
    "            \"encoder_input_ids\":encoder_input_ids,\n",
    "            \"decoder_input_ids\":decoder_input_ids,\n",
    "            \"labels\":labels,\n",
    "            \"encoder_mask\":create_padding_mask(encoder_input_ids, pad_id),\n",
    "            \"decoder_mask\":create_padding_mask(decoder_input_ids, pad_id)\n",
    "        }\n",
    "    \n",
    "    return collate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31d8424fc3771959"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample(model, tokenizer):\n",
    "    model.eval()\n",
    "    english = \"Several years ago here at TED, Peter Skillman  introduced a design challenge  called the marshmallow challenge.\"\n",
    "    chinese = \"几年前，在这里的 TED 上，Peter Skillman 提出了一个名为“棉花糖挑战”的设计挑战。\"\n",
    "    \n",
    "    #pad english\n",
    "    src_ids = tokenizer.encode(english).ids+[tokenizer.token_to_id(\"<pad>\")] * (\n",
    "            128-len(tokenizer.encode(english).ids)\n",
    "    )\n",
    "    src_ids = torch.tensor(src_ids,dtype=torch.int64)\n",
    "    src_ids = src_ids.unsqueeze(0)\n",
    "    src_mask = create_padding_mask(src_ids, pad_id=0)\n",
    "    \n",
    "    sos_id = tokenizer.token_to_id(\"<s>\")\n",
    "    decoder_input = torch.tensor([[sos_id]], dtype=torch.int64)\n",
    "    \n",
    "    src_ids = src_ids.to(DEVICE)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    \n",
    "    while decoder_input.size(1)<128:\n",
    "        with torch.no_grad():\n",
    "            out = model(src_ids, decoder_input, src_mask = src_mask)\n",
    "        \n",
    "        _, next_word = torch.max(out[:,-1], dim=-1)\n",
    "        \n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1,1).type_as(src_ids).fill_(next_word.item()).to(DEVICE)], dim=1\n",
    "        )\n",
    "        \n",
    "        if next_word == tokenizer.token_to_id(\"</s>\"):\n",
    "            break\n",
    "    \n",
    "    output = tokenizer.decode(decoder_input[0].tolist(), skip_special_tokens=False)\n",
    "    \n",
    "    model.train()\n",
    "    print_color(f\"English Input: {english}\", color=\"cyan\")\n",
    "    print_color(f\"Chinese Output: {output}\", color=\"cyan\")\n",
    "    print_color(f\"Reference: {chinese}\", color=\"cyan\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9600c9e7b87d2713"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.Start Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d54b53da9d9bed5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.1Training Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a88e0690fa689d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    num_epochs:int = 100\n",
    "    batch_size = 512\n",
    "    debug:bool = False\n",
    "    total_steps:int = 10000\n",
    "    warmup_steps:int=4000\n",
    "    \n",
    "    lr = 5e-3\n",
    "    min_lr = 1e-5\n",
    "    betas: tuple[float, float] = field(default_factory=lambda: (0.9,0.98))\n",
    "    weight_decay:float=0.01\n",
    "    \n",
    "    device = get_device()\n",
    "    mixed_precision:bool =True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9d3380d0e8d099d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.2Training Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc20992ddccab64d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_config = TrainConfig()\n",
    "\n",
    "translation_dataset = TranslationDataset(\n",
    "    file_prefix=\"train\", \n",
    "    tokenizer=tokenizer, \n",
    "    max_seq_len=config.max_seq_len, \n",
    "    is_debug=False)\n",
    "\n",
    "collate_fn = make_translation_collate_fn(\n",
    "    pad_id=translation_dataset.pad_id,\n",
    "    sos_id=translation_dataset.sos_id,\n",
    "    eos_id=translation_dataset.eos_id,\n",
    "    max_len=config.max_seq_len, \n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    translation_dataset,\n",
    "    batch_size=train_config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if \"cuda\" in DEVICE.type else False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "def cycle_dataloader(dataloader):\n",
    "    while True:\n",
    "        for batch in dataloader:\n",
    "            yield batch\n",
    "            \n",
    "dataloader = cycle_dataloader(train_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad8ded3d8b4651c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_config.total_steps = train_config.num_epochs*math.ceil(\n",
    "    len(translation_dataset)/train_config.batch_size\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27ec48ca9bd021cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.2.1Learning Rate Scheduler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6697e9a9de4c17f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerLRScheduler:\n",
    "    def __init__(self, optimizer, d_model:int, warmup_steps:int =4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_num=0\n",
    "        \n",
    "    def step(self):\n",
    "        self.step_num+=1\n",
    "        lr = (self.d_model**-0.5)*min(self.step_num**-0.5, self.step_num*(self.warmup_steps**-1.5))\n",
    "        \n",
    "        for pg in self.optimizer.param_groups:\n",
    "            pg[\"lr\"]=lr\n",
    "            \n",
    "        return lr\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7799d6999c783476"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.2.2Start Training Transformer\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0610173cda8370f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_config = ModelConfig()\n",
    "model = Transformer(model_config)\n",
    "model = model.to(train_config.device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.0, ignore_index=translation_dataset.pad_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = train_config.lr, betas=train_config.betas, eps=1e-9)\n",
    "scheduler = TransformerLRScheduler(optimizer, \n",
    "                                   d_model=model_config.d_model, warmup_steps= train_config.warmup_steps)\n",
    "\n",
    "print(\"Start training\")\n",
    "losses=[]\n",
    "\n",
    "step=0\n",
    "for epoch in range(train_config.num_epochs):\n",
    "    p_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{train_config.num_epochs}\")\n",
    "    epoch_loss=0.0\n",
    "    \n",
    "    for batch in p_bar:\n",
    "        step+=1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # One training step\n",
    "        # ----------------------------------------\n",
    "        encoder_input_ids = batch[\"encoder_input_ids\"].to(train_config.device)\n",
    "        decoder_input_ids = batch[\"decoder_input_ids\"].to(train_config.device)\n",
    "        enc_mask = batch[\"encoder_mask\"].to(train_config.device)\n",
    "        dec_mask = batch[\"decoder_mask\"].to(train_config.device)\n",
    "        labels = batch[\"labels\"].to(train_config.device)\n",
    "        \n",
    "        with torch.autocast(\n",
    "            device_type=train_config.device.type, enabled=train_config.mixed_precision, dtype=torch.bfloat16\n",
    "        ):\n",
    "            logits = model(encoder_input_ids, decoder_input_ids, src_mask=enc_mask, tgt_mask = dec_mask)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # Update parameters\n",
    "        # ----------------------------------------\n",
    "        lr = scheduler.step()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),2.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # Log training information\n",
    "        # ----------------------------------------\n",
    "        epoch_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        p_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{lr:.6f}\"})\n",
    "        \n",
    "        del loss, logits, encoder_input_ids, decoder_input_ids, enc_mask, dec_mask, labels\n",
    "        clear_cuda()\n",
    "        \n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    print_color(f\"Epoch {epoch + 1} completed. Average Loss: {avg_epoch_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ea042a9719c4d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.3Plot Loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9629d8fded1310c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.set_context(\"talk\")  # larger labels\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, label=\"Step Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Per Step\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_curve.png\", dpi=300, bbox_inches=\"tight\")  # high-res PNG\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e67b55f986a5448"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample(model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4cda2bf2b32b5b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
